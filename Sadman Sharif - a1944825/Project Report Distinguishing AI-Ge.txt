# Language Detection: Human vs AI Text

## Project Overview
This project addresses the task of distinguishing AI-generated text from human-written text using embeddings from `roberta-base-openai-detector`.  
We combine exploratory data analysis (EDA), engineered features, and a hybrid deep learning model to achieve strong performance on a Kaggle-style classification benchmark.

---

## Dataset
- **Train Data**:
  - `train_human.npy` → 8,161 samples of human text embeddings.
  - `train_ai.npy` → 8,161 samples of AI-generated text embeddings.
  - Balanced: 50/50 Human vs AI.
- **Validation Data**:
  - `validation.jsonl` → 220 usable samples, shape `(100, 768)`.
- **Test Data**:
  - Provided as `test_features.jsonl`, used for leaderboard submission.

Each sample is a `(100, 768)` embedding (100 tokens × 768 dimensions).

---

## Exploratory Data Analysis (EDA)
Key findings from multi-step analysis:

1. **Embedding Norms (L2)**  
   - AI embeddings have slightly higher mean norms (4.64 vs 4.25).  
   - AI shows stronger front-loading (early tokens carry more information).

2. **PCA Projection (2D)**  
   - Human embeddings: more spread out (greater variability).  
   - AI embeddings: compressed into a cone-like cluster (more deterministic).

3. **Cosine Similarity to Centroids**  
   - AI tightly clusters around its centroid.  
   - Human samples show more variance and diversity.

4. **Validation Distribution**  
   - Statistically closer to Human, but with AI-like traits (MMD, cosine heatmaps).

5. **Advanced EDA**  
   - UMAP/t-SNE: Human and AI heavily overlap; Validation sits between them.  
   - Variance and Sharpness: AI = sharper, more cohesive; Human = spread out.  
   - Cross-Class Similarity: High overlap (>0.97), making separation challenging.

---

## Feature Engineering
We designed interpretable tabular features to augment embeddings:
- Variance Masking → removes top-K noisy dimensions.  
- Segment Norms → captures early/mid/late token energy (AI front-loading).  
- Token Variance → measures intra-sample diversity.  
- Sharpness (Cosine Entropy) → quantifies semantic focus vs spread.  
- Centroid Similarity → cosine similarity to Human/AI prototypes.

---

## Model: UltraHybrid-Balanced++ v2
A custom hybrid model combining:
- Transformer branch (contextual encoding with AttentionPooling).  
- CNN branch (multi-scale Depthwise-Separable Conv + GLU + SE).  
- BiGRU branch (sequential dynamics with attention pooling).  
- FeatureGate (learns how much to trust engineered features).  
- Fusion Layer with DropPath regularization.  
- Multi-sample dropout for smoother logits.

**Training setup**:
- Optimizer: AdamW  
- Scheduler: OneCycleLR (warmup + cosine decay)  
- Loss: BCEWithLogitsLoss  
- MixUp augmentation (p=0.35, α=0.4)  
- Early stopping on validation AUC

---

## Results
Best model (Epoch 12):

| Split   | AUC   | ACC   | Precision | Recall | F1   |
|---------|-------|-------|-----------|--------|------|
| Train   | 0.9966 | 0.972 | 0.973     | 0.974  | 0.973 |
| Val     | 0.9625 | 0.895 | 0.868     | 0.931  | 0.898 |

- High recall (0.93) → good at catching AI texts.  
- Precision lower (0.868) → room for calibration or threshold tuning.  

---

## Inference and Submission
1. Parse test JSONL into embeddings `(N, 100, 768)`.  
2. Apply variance masking, feature engineering, and scaling.  
3. Pass through trained model to produce per-sentence probabilities.  
4. Pool into paragraph-level probability using logit averaging.  
5. Write submission file in the following format:  
