{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8a34c6",
   "metadata": {},
   "source": [
    "#A) Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db40605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# A) IMPORTS & SAFE CONFIG\n",
    "# ==========================================\n",
    "import os, math, json, random, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- device & seeding (robust to stale CUDA states) ---\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    try:\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    except RuntimeError as e:\n",
    "        print(\"CUDA seed failed; using CPU. Error:\", repr(e))\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# --- paths ---\n",
    "HUMAN_PATH = \"data/train/train_human.npy\"\n",
    "AI_PATH    = \"data/train/train_ai.npy\"\n",
    "ARTIFACTS  = Path(\"artifacts\"); ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- preprocessing ---\n",
    "MASK_TOPK_VAR_DIMS = 64      # zero-out top-K high-variance channels\n",
    "SEGMENTS           = 3       # early/mid/late splits\n",
    "\n",
    "# --- model capacity (UltraHybrid-Balanced++) ---\n",
    "D_IN     = 768\n",
    "D_MODEL  = 224\n",
    "N_HEADS  = 4\n",
    "N_LAYERS = 2\n",
    "D_GRU    = 160                 # per-direction\n",
    "CNN_KS   = (3, 5, 7)\n",
    "CNN_OUT  = 96                  # per-kernel\n",
    "\n",
    "# --- regularization ---\n",
    "DROPOUT      = 0.35\n",
    "GAUSS_NOISE  = 0.05            # std for Gaussian noise on tokens\n",
    "CHANNEL_DROP = 0.10            # SpatialDropout1D-style channel dropout\n",
    "\n",
    "# --- training ---\n",
    "BATCH_SIZE   = 128\n",
    "EPOCHS       = 30\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_FRAC  = 0.10\n",
    "ES_PATIENCE  = 6\n",
    "\n",
    "# --- mixup (both tokens & features) ---\n",
    "USE_MIXUP    = True\n",
    "MIXUP_P      = 0.35\n",
    "MIXUP_ALPHA  = 0.4             # Beta(alpha, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36badaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# B) DATA UTILS & ENGINEERED FEATURES\n",
    "# ==========================================\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def variance_mask(X: np.ndarray, topk: int) -> np.ndarray:\n",
    "    \"\"\"Zero-out top-K highest-variance channels across tokens+samples.\"\"\"\n",
    "    if topk is None or topk <= 0:\n",
    "        return X\n",
    "    flat = X.reshape(-1, X.shape[-1])            # (N*T, D)\n",
    "    var  = flat.var(axis=0)                      # (D,)\n",
    "    idx  = np.argsort(var)[::-1][:topk]\n",
    "    Xc   = X.copy()\n",
    "    Xc[..., idx] = 0.0\n",
    "    return Xc\n",
    "\n",
    "def segment_norms(X: np.ndarray, segments: int = 3) -> np.ndarray:\n",
    "    \"\"\"Mean L2 norm per segment (early/mid/late) — (N, segments).\"\"\"\n",
    "    N, T, D = X.shape\n",
    "    split = T // segments\n",
    "    chunks = []\n",
    "    for s in range(segments):\n",
    "        a = s*split\n",
    "        b = (s+1)*split if s < segments-1 else T\n",
    "        seg = X[:, a:b, :]\n",
    "        chunks.append(np.linalg.norm(seg, axis=2).mean(axis=1))\n",
    "    return np.stack(chunks, axis=1).astype(np.float32)\n",
    "\n",
    "def token_variance(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Average variance across tokens — (N,1).\"\"\"\n",
    "    return X.var(axis=1).mean(axis=1, keepdims=True).astype(np.float32)\n",
    "\n",
    "def sharpness_cosine(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Std of cosine distance between each token and the sample mean — (N,1).\"\"\"\n",
    "    from scipy.spatial.distance import cosine as cosdist\n",
    "    vals = []\n",
    "    for sample in X:\n",
    "        mu = sample.mean(axis=0)\n",
    "        if np.linalg.norm(mu) < 1e-8:\n",
    "            vals.append(0.0); continue\n",
    "        d = [cosdist(tok, mu) for tok in sample]\n",
    "        vals.appe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0a9f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: (13873, 100, 768) (2449, 100, 768) | Feats: (13873, 3076) (2449, 3076)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# C) SPLIT, CENTROIDS & SCALING (TRAIN-ONLY FIT)\n",
    "# ==========================================\n",
    "# helper: class centroids from mean pooled tokens\n",
    "def compute_centroids(X: np.ndarray, y: np.ndarray) -> dict:\n",
    "    \"\"\"Compute per-class centroids over mean pooled token embeddings.\"\"\"\n",
    "    Xm = X.mean(axis=1)  # mean over 100 tokens\n",
    "    return {\n",
    "        \"human\": Xm[y == 0].mean(axis=0),\n",
    "        \"ai\": Xm[y == 1].mean(axis=0)\n",
    "    }\n",
    "\n",
    "# helper: engineered features\n",
    "def build_features(X: np.ndarray, cents: dict, segments: int = 4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build engineered features from token sequences.\n",
    "    - segment means\n",
    "    - cosine sim to centroids\n",
    "    - variance and norms\n",
    "    \"\"\"\n",
    "    N, T, D = X.shape\n",
    "    feats = []\n",
    "\n",
    "    # mean over all tokens\n",
    "    Xm = X.mean(axis=1)\n",
    "    feats.append(Xm)\n",
    "\n",
    "    # cosine sim to class centroids\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    for c in [\"human\", \"ai\"]:\n",
    "        sim = cosine_similarity(Xm, cents[c][None, :])[:, 0]\n",
    "        feats.append(sim[:, None])\n",
    "\n",
    "    # split sequence into segments\n",
    "    seg_size = T // segments\n",
    "    for s in range(segments):\n",
    "        seg = X[:, s * seg_size:(s + 1) * seg_size, :].mean(axis=1)\n",
    "        feats.append(seg)\n",
    "\n",
    "    # variance + L2 norms\n",
    "    feats.append(X.var(axis=(1, 2))[:, None])\n",
    "    feats.append(np.linalg.norm(Xm, axis=1)[:, None])\n",
    "\n",
    "    return np.concatenate(feats, axis=1).astype(np.float32)\n",
    "\n",
    "\n",
    "# load arrays + labels\n",
    "X_h = np.load(HUMAN_PATH).astype(np.float32)    # (Nh,100,768)\n",
    "X_a = np.load(AI_PATH).astype(np.float32)       # (Na,100,768)\n",
    "y_h = np.zeros(len(X_h), dtype=np.int64)\n",
    "y_a = np.ones(len(X_a),  dtype=np.int64)\n",
    "\n",
    "X_all = np.concatenate([X_h, X_a], axis=0)\n",
    "y_all = np.concatenate([y_h, y_a], axis=0)\n",
    "\n",
    "# mask once for all\n",
    "X_all_m = variance_mask(X_all, MASK_TOPK_VAR_DIMS)\n",
    "\n",
    "# stratified split\n",
    "Xt_tr, Xt_va, y_tr, y_va = train_test_split(\n",
    "    X_all_m, y_all, test_size=0.15, random_state=SEED, stratify=y_all\n",
    ")\n",
    "\n",
    "# centroids on TRAIN only\n",
    "cents_tr = compute_centroids(Xt_tr, y_tr)\n",
    "\n",
    "# engineered features\n",
    "Xf_tr = build_features(Xt_tr, cents_tr, segments=SEGMENTS)\n",
    "Xf_va = build_features(Xt_va, cents_tr, segments=SEGMENTS)\n",
    "\n",
    "# scale features on TRAIN only\n",
    "scaler = StandardScaler()\n",
    "Xf_tr_s = scaler.fit_transform(Xf_tr).astype(np.float32)\n",
    "Xf_va_s = scaler.transform(Xf_va).astype(np.float32)\n",
    "\n",
    "# label sanity (avoid CUDA asserts)\n",
    "assert set(np.unique(y_tr)).issubset({0,1})\n",
    "assert set(np.unique(y_va)).issubset({0,1})\n",
    "\n",
    "print(\"Tokens:\", Xt_tr.shape, Xt_va.shape, \"| Feats:\", Xf_tr_s.shape, Xf_va_s.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bb45c",
   "metadata": {},
   "source": [
    "#D) UltraHybrid-Lite++ model (Transformer + CNN + BiGRU), with robust regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cbc70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# D) MODEL: UltraHybrid-Balanced++ (v2)\n",
    "# ==========================================\n",
    "# Drop-in replacement for the class you posted. Keeps interface:\n",
    "#   model = UltraHybridBalancedPPv2(feat_dim)\n",
    "#   logits = model(xtokens, xfeats)  # (B,)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- small building blocks ----------\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.0):\n",
    "        super().__init__(); self.std = float(std)\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.std <= 0: return x\n",
    "        return x + torch.randn_like(x) * self.std\n",
    "\n",
    "class ChannelDropout1D(nn.Module):\n",
    "    \"\"\"Drop entire embedding channels (SpatialDropout1D analogue).\"\"\"\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__(); self.p = float(p)\n",
    "    def forward(self, x):  # x: (B,T,D)\n",
    "        if not self.training or self.p <= 0: return x\n",
    "        B,T,D = x.shape\n",
    "        mask = (torch.rand(B,1,D, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic depth; drops residual branch per-sample.\"\"\"\n",
    "    def __init__(self, p: float = 0.0):\n",
    "        super().__init__(); self.p = float(p)\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p <= 0: return x\n",
    "        keep = 1 - self.p\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        mask = x.new_empty(shape).bernoulli_(keep).div(keep)\n",
    "        return x * mask\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Single learnable query attends over tokens, returns pooled vector.\n",
    "    Much more robust than mean/max pooling under distribution shift.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int = 4, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.q = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):          # x: (B,T,D)\n",
    "        B = x.size(0)\n",
    "        q = self.q.expand(B, -1, -1)         # (B,1,D)\n",
    "        out, _ = self.attn(q, x, x, need_weights=False)  # (B,1,D)\n",
    "        return self.ln(out.squeeze(1))       # (B,D)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-Excitation for channel reweighting.\"\"\"\n",
    "    def __init__(self, c: int, r: int = 8):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c, max(1, c//r)), nn.SiLU(),\n",
    "            nn.Linear(max(1, c//r), c), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):           # x: (B,C)\n",
    "        return x * self.fc(x)\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise + Pointwise conv with GLU gating.\n",
    "    In: (B, D_model, T)  Out: (B, C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, c_out: int, k: int, dropout: float):\n",
    "        super().__init__()\n",
    "        padding = k // 2\n",
    "        self.dw = nn.Conv1d(d_in, d_in, kernel_size=k, padding=padding, groups=d_in, bias=False)\n",
    "        self.pw = nn.Conv1d(d_in, 2*c_out, kernel_size=1, bias=True)   # 2*c_out for GLU\n",
    "        self.bn = nn.BatchNorm1d(2*c_out)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.se = SEBlock(c_out)\n",
    "    def forward(self, x):           # x: (B,D,T)\n",
    "        h = self.dw(x)\n",
    "        h = self.pw(h)\n",
    "        h = self.bn(h)\n",
    "        a, b = torch.chunk(h, 2, dim=1)      # GLU\n",
    "        h = a * torch.sigmoid(b)             # (B,C_out,T)\n",
    "        h = F.adaptive_max_pool1d(h, 1).squeeze(-1)  # (B,C_out)\n",
    "        h = self.drop(h)\n",
    "        return self.se(h)\n",
    "\n",
    "class FeatureGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns how much to trust engineered features.\n",
    "    Applies LN -> small MLP -> sigmoid gate, then scales features.\n",
    "    \"\"\"\n",
    "    def __init__(self, fdim: int):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(fdim)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fdim, max(8, fdim//2)), nn.GELU(),\n",
    "            nn.Linear(max(8, fdim//2), fdim), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, f):           # (B,F)\n",
    "        g = self.gate(self.ln(f))\n",
    "        return f * g\n",
    "\n",
    "# ---------- main model ----------\n",
    "class UltraHybridBalancedPPv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Upgrades over your UltraHybridBalancedPP:\n",
    "      • Transformer branch w/ AttentionPooling (learned query)\n",
    "      • CNN branch: Depthwise-Separable + GLU + SE (per kernel)\n",
    "      • BiGRU branch w/ AttentionPooling\n",
    "      • FeatureGate on engineered features\n",
    "      • DropPath on residual-like fusions\n",
    "      • Multi-sample dropout in the head for smoother logits\n",
    "    Returns logits (use BCEWithLogitsLoss).\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim: int,\n",
    "                 d_in: int = 768, d_model: int = 224,\n",
    "                 n_heads: int = 4, n_layers: int = 2,\n",
    "                 cnn_kernels=(3,5,7), cnn_out: int = 96,\n",
    "                 gru_h: int = 160, dropout: float = 0.35,\n",
    "                 channel_drop: float = 0.10, gauss_noise: float = 0.05,\n",
    "                 drop_path: float = 0.05, ms_dropout_samples: int = 4):\n",
    "        super().__init__()\n",
    "        self.ms_dropout_samples = ms_dropout_samples\n",
    "\n",
    "        # input regularizers\n",
    "        self.noise = GaussianNoise(gauss_noise)\n",
    "        self.cdrop = ChannelDropout1D(channel_drop)\n",
    "\n",
    "        # shared projection\n",
    "        self.proj  = nn.Linear(d_in, d_model)\n",
    "\n",
    "        # ---- Transformer branch ----\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dropout=dropout,\n",
    "            activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.tr_encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.tr_pool    = AttentionPooling(d_model, n_heads=n_heads, dropout=dropout)\n",
    "\n",
    "        # ---- CNN branch (multi-kernel DS-Conv + GLU + SE) ----\n",
    "        self.cnn_blocks = nn.ModuleList([\n",
    "            DepthwiseSeparableConv1d(d_model, cnn_out, k, dropout) for k in cnn_kernels\n",
    "        ])\n",
    "        self.cnn_ln  = nn.LayerNorm(len(cnn_kernels)*cnn_out)\n",
    "\n",
    "        # ---- BiGRU branch ----\n",
    "        self.gru  = nn.GRU(d_model, gru_h, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.gru_pool = AttentionPooling(2*gru_h, n_heads=min(4, n_heads), dropout=dropout)\n",
    "\n",
    "        # ---- Engineered features gate ----\n",
    "        self.fgate = FeatureGate(feat_dim)\n",
    "\n",
    "        # ---- Fusion & head ----\n",
    "        fusion_dim = d_model + (len(cnn_kernels)*cnn_out) + (2*gru_h) + feat_dim\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.pre_head  = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, 256), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64), nn.GELU()\n",
    "        )\n",
    "        # multi-sample dropout heads averaged at forward\n",
    "        self.head_dropout = nn.Dropout(dropout)\n",
    "        self.head_last    = nn.Linear(64, 1)  # logits\n",
    "\n",
    "    def forward(self, xtokens: torch.Tensor, xfeats: torch.Tensor):\n",
    "        # xtokens: (B,T,768), xfeats: (B,F)\n",
    "        # input reg\n",
    "        x = self.noise(xtokens)\n",
    "        x = self.cdrop(x)\n",
    "        x = self.proj(x)                               # (B,T,D)\n",
    "\n",
    "        # transformer branch\n",
    "        t = self.tr_encoder(x)                         # (B,T,D)\n",
    "        t = self.tr_pool(t)                            # (B,D)\n",
    "\n",
    "        # cnn branch\n",
    "        xc = x.transpose(1,2)                          # (B,D,T)\n",
    "        c_parts = [blk(xc) for blk in self.cnn_blocks] # list of (B,C_out)\n",
    "        c = torch.cat(c_parts, dim=1)                  # (B,sumC)\n",
    "        c = self.cnn_ln(c)\n",
    "\n",
    "        # bigru branch\n",
    "        g, _ = self.gru(x)                             # (B,T,2*H)\n",
    "        g = self.gru_pool(g)                           # (B,2*H)\n",
    "\n",
    "        # gated engineered features\n",
    "        f = self.fgate(xfeats)                         # (B,F)\n",
    "\n",
    "        # fuse + light residual via DropPath\n",
    "        z = torch.cat([t, c, g, f], dim=1)\n",
    "        z = z + self.drop_path(z)                      # stochastic depth-like perturbation\n",
    "        z = self.pre_head(z)                           # (B,64)\n",
    "\n",
    "        # multi-sample dropout (averaged logits for smoother training)\n",
    "        if self.training and self.ms_dropout_samples > 1:\n",
    "            logits = 0.0\n",
    "            for _ in range(self.ms_dropout_samples):\n",
    "                logits = logits + self.head_last(self.head_dropout(z))\n",
    "            logits = logits / float(self.ms_dropout_samples)\n",
    "            return logits.squeeze(1)\n",
    "        else:\n",
    "            return self.head_last(self.head_dropout(z)).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51fdb2",
   "metadata": {},
   "source": [
    "#E) training utilities & loop (with BCEWithLogitsLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8adaecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# E) TRAINING UTILITIES & LOOP\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ---- dataset ----\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, Xt, Xf, y):\n",
    "        self.Xt = torch.from_numpy(Xt).float()\n",
    "        self.Xf = torch.from_numpy(Xf).float()\n",
    "        self.y  = torch.from_numpy(y.astype(np.float32))\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xt[i], self.Xf[i], self.y[i]\n",
    "\n",
    "# ---- mixup over tokens+features+labels ----\n",
    "def mixup_batch(xt, xf, y, alpha: float):\n",
    "    if alpha <= 0:\n",
    "        return xt, xf, None\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(xt.size(0), device=xt.device)\n",
    "    xtm = lam*xt + (1-lam)*xt[idx]\n",
    "    xfm = lam*xf + (1-lam)*xf[idx]\n",
    "    return xtm, xfm, (y, y[idx], lam)\n",
    "\n",
    "# ---- metrics from logits ----\n",
    "def metrics_from_logits(y_true_np, logits_np, thr=0.5):\n",
    "    p = 1.0 / (1.0 + np.exp(-logits_np))\n",
    "    yhat = (p >= thr).astype(int)\n",
    "    return dict(\n",
    "        auc = float(roc_auc_score(y_true_np, p)),\n",
    "        ap  = float(average_precision_score(y_true_np, p)),\n",
    "        acc = float(accuracy_score(y_true_np, yhat)),\n",
    "        prec= float(precision_score(y_true_np, yhat, zero_division=0)),\n",
    "        rec = float(recall_score(y_true_np, yhat)),\n",
    "        f1  = float(f1_score(y_true_np, yhat)),\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    logits_all, y_all = [], []\n",
    "    for xt, xf, yy in loader:\n",
    "        xt, xf = xt.to(device), xf.to(device)\n",
    "        logits = model(xt, xf).cpu().numpy()\n",
    "        logits_all.append(logits); y_all.append(yy.numpy())\n",
    "    logits_all = np.concatenate(logits_all)\n",
    "    y_all      = np.concatenate(y_all)\n",
    "    return metrics_from_logits(y_all, logits_all, thr=0.5), logits_all, y_all\n",
    "\n",
    "def train_model(Xt_tr, Xf_tr, y_tr, Xt_va, Xf_va, y_va,\n",
    "                batch_size=128, epochs=30, lr=2e-4,\n",
    "                weight_decay=1e-4, warmup_frac=0.10,\n",
    "                use_mixup=True, mixup_p=0.35, mixup_alpha=0.4,\n",
    "                es_patience=6, device=None):\n",
    "    \"\"\"\n",
    "    Train UltraHybridBalancedPPv2 with BCEWithLogitsLoss,\n",
    "    OneCycleLR warmup-like schedule, early stopping on val AUC,\n",
    "    and (optionally) mixup.\n",
    "    \"\"\"\n",
    "    assert device is not None, \"Pass DEVICE\"\n",
    "    tr_ds = LangDataset(Xt_tr, Xf_tr, y_tr)\n",
    "    va_ds = LangDataset(Xt_va, Xf_va, y_va)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "    va_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    feat_dim = Xf_tr.shape[1]\n",
    "    model = UltraHybridBalancedPPv2(\n",
    "        feat_dim=feat_dim,\n",
    "        d_in=768, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
    "        cnn_kernels=CNN_KS, cnn_out=CNN_OUT, gru_h=D_GRU,\n",
    "        dropout=DROPOUT, channel_drop=CHANNEL_DROP,\n",
    "        gauss_noise=GAUSS_NOISE, drop_path=0.05, ms_dropout_samples=4\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=lr, steps_per_epoch=max(1,len(tr_ld)), epochs=epochs, pct_start=warmup_frac\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_auc, best_state, patience = -1.0, None, es_patience\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xt, xf, yy in tr_ld:\n",
    "            xt, xf, yy = xt.to(device), xf.to(device), yy.to(device)\n",
    "\n",
    "            if use_mixup and np.random.rand() < mixup_p:\n",
    "                xtm, xfm, mix = mixup_batch(xt, xf, yy, alpha=mixup_alpha)\n",
    "                if mix is None:\n",
    "                    logits = model(xt, xf); loss = criterion(logits, yy)\n",
    "                else:\n",
    "                    y_a, y_b, lam = mix\n",
    "                    logits = model(xtm, xfm)\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            else:\n",
    "                logits = model(xt, xf)\n",
    "                loss = criterion(logits, yy)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            opt.step(); sched.step()\n",
    "\n",
    "        # ---- end epoch: evaluate ----\n",
    "        tr_metrics, _, _ = evaluate(model, tr_ld, device)\n",
    "        va_metrics, _, _ = evaluate(model, va_ld, device)\n",
    "        print(f\"Epoch {ep:02d} | \"\n",
    "              f\"TR AUC {tr_metrics['auc']:.4f} | TR ACC {tr_metrics['acc']:.4f} | TR F1 {tr_metrics['f1']:.4f} || \"\n",
    "              f\"VA AUC {va_metrics['auc']:.4f} | VA ACC {va_metrics['acc']:.4f} | VA F1 {va_metrics['f1']:.4f}\")\n",
    "\n",
    "        # ---- early stopping on val AUC ----\n",
    "        if va_metrics[\"auc\"] > best_auc:\n",
    "            best_auc = va_metrics[\"auc\"]\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            patience = es_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "\n",
    "    # final metrics\n",
    "    tr_metrics, _, _ = evaluate(model, tr_ld, device)\n",
    "    va_metrics, _, _ = evaluate(model, va_ld, device)\n",
    "\n",
    "    # save artifacts if the globals exist\n",
    "    try:\n",
    "        torch.save(model.state_dict(), ARTIFACTS/\"ultrahybrid_balanced_pp_v2.pt\")\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "    return model, tr_metrics, va_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab0f10",
   "metadata": {},
   "source": [
    "#F) Run training and print full metrics (train & validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b0694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | TR AUC 0.8987 | TR ACC 0.8152 | TR F1 0.8133 || VA AUC 0.8920 | VA ACC 0.8073 | VA F1 0.8051\n",
      "Epoch 02 | TR AUC 0.9554 | TR ACC 0.8830 | TR F1 0.8808 || VA AUC 0.9466 | VA ACC 0.8763 | VA F1 0.8738\n",
      "Epoch 03 | TR AUC 0.9651 | TR ACC 0.9008 | TR F1 0.9015 || VA AUC 0.9542 | VA ACC 0.8877 | VA F1 0.8892\n",
      "Epoch 04 | TR AUC 0.9706 | TR ACC 0.9080 | TR F1 0.9105 || VA AUC 0.9565 | VA ACC 0.8930 | VA F1 0.8969\n",
      "Epoch 05 | TR AUC 0.9726 | TR ACC 0.9141 | TR F1 0.9131 || VA AUC 0.9530 | VA ACC 0.8795 | VA F1 0.8795\n",
      "Epoch 06 | TR AUC 0.9801 | TR ACC 0.9294 | TR F1 0.9300 || VA AUC 0.9591 | VA ACC 0.8934 | VA F1 0.8954\n",
      "Epoch 07 | TR AUC 0.9850 | TR ACC 0.9402 | TR F1 0.9401 || VA AUC 0.9607 | VA ACC 0.8918 | VA F1 0.8921\n",
      "Epoch 08 | TR AUC 0.9875 | TR ACC 0.9407 | TR F1 0.9422 || VA AUC 0.9604 | VA ACC 0.8906 | VA F1 0.8955\n",
      "Epoch 09 | TR AUC 0.9908 | TR ACC 0.9501 | TR F1 0.9511 || VA AUC 0.9608 | VA ACC 0.8926 | VA F1 0.8968\n",
      "Epoch 10 | TR AUC 0.9931 | TR ACC 0.9577 | TR F1 0.9586 || VA AUC 0.9607 | VA ACC 0.8877 | VA F1 0.8918\n",
      "Epoch 11 | TR AUC 0.9949 | TR ACC 0.9682 | TR F1 0.9686 || VA AUC 0.9593 | VA ACC 0.8889 | VA F1 0.8919\n",
      "Epoch 12 | TR AUC 0.9966 | TR ACC 0.9722 | TR F1 0.9726 || VA AUC 0.9625 | VA ACC 0.8947 | VA F1 0.8983\n",
      "Epoch 13 | TR AUC 0.9970 | TR ACC 0.9743 | TR F1 0.9743 || VA AUC 0.9551 | VA ACC 0.8840 | VA F1 0.8841\n",
      "Epoch 14 | TR AUC 0.9980 | TR ACC 0.9825 | TR F1 0.9825 || VA AUC 0.9586 | VA ACC 0.8893 | VA F1 0.8900\n",
      "Epoch 15 | TR AUC 0.9986 | TR ACC 0.9872 | TR F1 0.9872 || VA AUC 0.9581 | VA ACC 0.8873 | VA F1 0.8875\n",
      "Epoch 16 | TR AUC 0.9987 | TR ACC 0.9879 | TR F1 0.9879 || VA AUC 0.9546 | VA ACC 0.8857 | VA F1 0.8858\n",
      "Epoch 17 | TR AUC 0.9992 | TR ACC 0.9891 | TR F1 0.9892 || VA AUC 0.9584 | VA ACC 0.8865 | VA F1 0.8889\n",
      "Epoch 18 | TR AUC 0.9994 | TR ACC 0.9907 | TR F1 0.9907 || VA AUC 0.9563 | VA ACC 0.8865 | VA F1 0.8840\n",
      "Early stopping.\n",
      "\n",
      "[TRAIN]\n",
      "ROC-AUC: 0.99664 | AP: 0.99673\n",
      "ACC: 0.97222 | PREC: 0.95922 | REC: 0.98641 | F1: 0.97262\n",
      "\n",
      "[VALIDATION]\n",
      "ROC-AUC: 0.96248 | AP: 0.96061\n",
      "ACC: 0.89465 | PREC: 0.86814 | REC: 0.93056 | F1: 0.89826\n",
      "\n",
      "Artifacts saved to: D:\\last_ai_ml\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# F) RUN TRAINING & REPORT METRICS\n",
    "# ==========================================\n",
    "model, tr_m, va_m = train_model(\n",
    "    Xt_tr, Xf_tr_s, y_tr,\n",
    "    Xt_va, Xf_va_s, y_va,\n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY, warmup_frac=WARMUP_FRAC,\n",
    "    use_mixup=USE_MIXUP, mixup_p=MIXUP_P, mixup_alpha=MIXUP_ALPHA,\n",
    "    es_patience=ES_PATIENCE, device=DEVICE\n",
    ")\n",
    "\n",
    "def pretty(m, name):\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"ROC-AUC: {m['auc']:.5f} | AP: {m['ap']:.5f}\")\n",
    "    print(f\"ACC: {m['acc']:.5f} | PREC: {m['prec']:.5f} | REC: {m['rec']:.5f} | F1: {m['f1']:.5f}\")\n",
    "\n",
    "pretty(tr_m, \"TRAIN\")\n",
    "pretty(va_m, \"VALIDATION\")\n",
    "\n",
    "# (optional) persist scaler & centroids for later inference\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(scaler, ARTIFACTS/\"scaler.pkl\")\n",
    "    np.save(ARTIFACTS/\"centroid_h.npy\", cents_tr[\"human\"])\n",
    "    np.save(ARTIFACTS/\"centroid_a.npy\", cents_tr[\"ai\"])\n",
    "    print(\"\\nArtifacts saved to:\", ARTIFACTS.resolve())\n",
    "except Exception as _:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10eac0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation threshold (by F1): 0.600 | F1=0.9005\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# G) OPTIONAL — THRESHOLD TUNING ON VALIDATION\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_logits(model):\n",
    "    model.eval()\n",
    "    dl = DataLoader(LangDataset(Xt_va, Xf_va_s, y_va), batch_size=256, shuffle=False)\n",
    "    logits, ys = [], []\n",
    "    for xt, xf, yy in dl:\n",
    "        xt, xf = xt.to(DEVICE), xf.to(DEVICE)\n",
    "        logits.append(model(xt, xf).cpu().numpy())\n",
    "        ys.append(yy.numpy())\n",
    "    return np.concatenate(logits), np.concatenate(ys)\n",
    "\n",
    "def tune_threshold(logits, y_true, grid=np.linspace(0.1, 0.9, 33)):\n",
    "    p = 1/(1+np.exp(-logits))\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for thr in grid:\n",
    "        yhat = (p >= thr).astype(int)\n",
    "        f1 = f1_score(y_true, yhat)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return best_thr, best_f1\n",
    "\n",
    "logits_va, y_va_true = val_logits(model)\n",
    "best_thr, best_f1 = tune_threshold(logits_va, y_va_true)\n",
    "print(f\"\\nBest validation threshold (by F1): {best_thr:.3f} | F1={best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff69811",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# H) TEST → SUBMISSION (paragraph-level, 1 row per id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea95a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file: data/test/test_features.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs parsed: 180\n",
      "   id    y_prob\n",
      "0  15  0.010840\n",
      "1  16  0.031168\n",
      "2  17  0.065514\n",
      "3  18  0.479125\n",
      "4  19  0.126789\n",
      "5  21  0.021771\n",
      "6  24  0.049797\n",
      "7  25  0.402383\n",
      "8  27  0.128001\n",
      "9  29  0.289021\n",
      "\n",
      "Saved: D:\\last_ai_ml\\artifacts\\submission_base_prob.csv\n",
      "Shape: (180, 2)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# H) TEST → SUBMISSION (PARAGRAPH-LEVEL, SINGLE CSV WITH y_prob)\n",
    "# ==========================================\n",
    "import os, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Required runtime objects\n",
    "# -----------------------------\n",
    "assert 'model' in globals(), \"Model not found. Run training first so `model` is defined.\"\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Output dir\n",
    "if 'ARTIFACTS' not in globals():\n",
    "    ARTIFACTS = Path(\"./outputs\")\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------\n",
    "# 1) Try to reuse your preprocessing\n",
    "#    (variance_mask, build_features,\n",
    "#     scaler, cents_tr, SEGMENTS, etc.)\n",
    "#    If missing, use safe fallbacks.\n",
    "# ------------------------------------\n",
    "USE_FALLBACKS = False\n",
    "if not all(k in globals() for k in ['build_features', 'scaler']):\n",
    "    USE_FALLBACKS = True\n",
    "\n",
    "if 'SEGMENTS' not in globals():\n",
    "    SEGMENTS = (33, 33, 34)   # default split used by many setups\n",
    "\n",
    "if 'MASK_TOPK_VAR_DIMS' not in globals():\n",
    "    MASK_TOPK_VAR_DIMS = None\n",
    "\n",
    "def _variance_mask_fallback(x, topk=None):\n",
    "    # No-op: just return input unchanged\n",
    "    return x\n",
    "\n",
    "variance_mask_fn = globals().get('variance_mask', _variance_mask_fallback)\n",
    "\n",
    "def _build_features_fallback(Xt_tokens, cents=None, segments=SEGMENTS):\n",
    "    \"\"\"\n",
    "    Minimal, robust features if your custom `build_features` is not available.\n",
    "    - Mean-pool 100x768 -> 768\n",
    "    - Early/Mid/Late segment L2-norm means -> 3\n",
    "    Output: (N, 771)\n",
    "    \"\"\"\n",
    "    # mean pool\n",
    "    pooled = Xt_tokens.mean(axis=1)  # (N, 768)\n",
    "\n",
    "    # segment L2 stats\n",
    "    e = Xt_tokens[:, :segments[0], :]\n",
    "    m = Xt_tokens[:, segments[0]:segments[0]+segments[1], :]\n",
    "    l = Xt_tokens[:, -segments[2]:, :]\n",
    "    def mean_l2(a): return np.linalg.norm(a, axis=2).mean(axis=1, keepdims=True)\n",
    "    seg = np.hstack([mean_l2(e), mean_l2(m), mean_l2(l)])  # (N, 3)\n",
    "\n",
    "    return np.hstack([pooled, seg]).astype(np.float32)\n",
    "\n",
    "build_features_fn = globals().get('build_features', _build_features_fallback)\n",
    "\n",
    "# Standardize features if a scaler is provided; else pass-through\n",
    "def _maybe_scale(Xf):\n",
    "    if 'scaler' in globals():\n",
    "        return globals()['scaler'].transform(Xf).astype(np.float32)\n",
    "    return Xf.astype(np.float32)\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) Locate test JSONL\n",
    "# ------------------------------------\n",
    "def find_test_jsonl():\n",
    "    for p in [\n",
    "        \"../data/test/test_features.jsonl\",\n",
    "        \"../data/test/test.jsonl\",\n",
    "        \"../data/test_features.jsonl\",\n",
    "        \"data/test/test_features.jsonl\",\n",
    "        \"data/test/test.jsonl\",\n",
    "    ]:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    files = glob.glob(\"**/*test*features*.jsonl\", recursive=True) or glob.glob(\"**/*.jsonl\", recursive=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"Could not find any test .jsonl file\")\n",
    "    return files[0]\n",
    "\n",
    "TEST_JSONL = find_test_jsonl()\n",
    "print(\"Test file:\", TEST_JSONL)\n",
    "\n",
    "# ------------------------------------\n",
    "# 3) Parse test JSONL grouped by paragraph id\n",
    "#    Returns list of (id_str, np.array (M,100,768))\n",
    "# ------------------------------------\n",
    "def parse_jsonl_grouped(path):\n",
    "    grouped = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            base_id = obj.get(\"id\", obj.get(\"guid\", obj.get(\"index\", i)))\n",
    "            feats = obj.get(\"features\")\n",
    "            if feats is None:\n",
    "                continue\n",
    "\n",
    "            arrs = []\n",
    "            if isinstance(feats, list):\n",
    "                for a in feats:\n",
    "                    a = np.asarray(a).squeeze()\n",
    "                    if a.shape == (100, 768):\n",
    "                        arrs.append(a.astype(np.float32))\n",
    "            else:\n",
    "                a = np.asarray(feats).squeeze()\n",
    "                if a.shape == (100, 768):\n",
    "                    arrs.append(a.astype(np.float32))\n",
    "\n",
    "            if len(arrs) == 0:\n",
    "                # Nothing usable on this line; skip with a light warn\n",
    "                # print(f\"[warn] skipping line {i} (no (100,768) chunks)\")\n",
    "                continue\n",
    "\n",
    "            X = np.stack(arrs, axis=0)  # (M,100,768)\n",
    "            grouped.append((str(base_id), X))\n",
    "    if not grouped:\n",
    "        raise ValueError(\"Parsed zero paragraphs from test JSONL.\")\n",
    "    return grouped\n",
    "\n",
    "grouped = parse_jsonl_grouped(TEST_JSONL)\n",
    "print(f\"Paragraphs parsed: {len(grouped)}\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 4) Inference helpers\n",
    "# ------------------------------------\n",
    "class _SmallSet(Dataset):\n",
    "    def __init__(self, Xt, Xf):\n",
    "        self.Xt = torch.from_numpy(Xt).float()\n",
    "        self.Xf = torch.from_numpy(Xf).float()\n",
    "        self.y  = torch.zeros(len(Xt), dtype=torch.float32)  # dummy\n",
    "    def __len__(self): return len(self.Xt)\n",
    "    def __getitem__(self, i): return self.Xt[i], self.Xf[i], self.y[i]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs_for_tokens(Xt_tokens):  # Xt_tokens: (M,100,768)\n",
    "    # same masking policy\n",
    "    Xt_m = variance_mask_fn(Xt_tokens, MASK_TOPK_VAR_DIMS)\n",
    "\n",
    "    # tabular features\n",
    "    Xf   = build_features_fn(Xt_m, globals().get('cents_tr', None), segments=SEGMENTS)\n",
    "    Xf_s = _maybe_scale(Xf)\n",
    "\n",
    "    dl = DataLoader(_SmallSet(Xt_m, Xf_s), batch_size=256, shuffle=False)\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for xt, xf, _ in dl:\n",
    "        xt, xf = xt.to(DEVICE), xf.to(DEVICE)\n",
    "        logits = model(xt, xf)             # expects logits\n",
    "        out.append(torch.sigmoid(logits).cpu().numpy())\n",
    "    return np.concatenate(out, axis=0)  # (M,)\n",
    "\n",
    "# ------------------------------------\n",
    "# 5) Paragraph aggregator (LOGIT-MEAN)\n",
    "#    Slightly sharper than plain mean\n",
    "# ------------------------------------\n",
    "def pool_paragraph_logit(probs, eps=1e-6, temp=1.0):\n",
    "    p = np.clip(probs, eps, 1 - eps)\n",
    "    logits = np.log(p) - np.log(1 - p)\n",
    "    m = logits.mean() / max(1e-6, temp)\n",
    "    return float(1 / (1 + np.exp(-m)))\n",
    "\n",
    "# If you want the plain average instead, use:\n",
    "# def pool_paragraph_mean(probs): return float(probs.mean())\n",
    "\n",
    "# ------------------------------------\n",
    "# 6) Run inference per paragraph\n",
    "# ------------------------------------\n",
    "records = []\n",
    "for pid, Xtok in grouped:\n",
    "    # per-sentence probabilities\n",
    "    p_sent = predict_probs_for_tokens(Xtok)\n",
    "    # single probability per paragraph\n",
    "    y_prob = pool_paragraph_logit(p_sent)   # <-- \"slightly sharper\" aggregator\n",
    "    records.append((pid, y_prob))\n",
    "\n",
    "# ------------------------------------\n",
    "# 7) Build EXACT submission and save ONE file\n",
    "# ------------------------------------\n",
    "sub = pd.DataFrame(records, columns=[\"id\", \"y_prob\"])\n",
    "\n",
    "# Cast id to int if possible (keeps your sample CSV look)\n",
    "sub[\"id\"] = pd.to_numeric(sub[\"id\"], errors=\"ignore\")\n",
    "\n",
    "# Sort for determinism\n",
    "sub = sub.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "OUT_PATH = ARTIFACTS / \"submission_base_prob.csv\"\n",
    "sub.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(sub.head(10))\n",
    "print(f\"\\nSaved: {OUT_PATH.resolve()}\")\n",
    "print(\"Shape:\", sub.shape)\n",
    "if USE_FALLBACKS:\n",
    "    print(\"[INFO] Used fallback feature pipeline (no custom scaler/build_features found).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
