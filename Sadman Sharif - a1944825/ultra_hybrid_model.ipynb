{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645d804f",
   "metadata": {},
   "source": [
    "#1) Imports, config, and seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c4cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UltraHybrid XL++ (Notebook Edition): Cell 1 / Config & Helpers ---\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, math, json, random, copy, warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, balanced_accuracy_score, average_precision_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "# ----------------- user-editable paths -----------------\n",
    "data_root_path = Path(r\"D:\\ML_CHALLANGE\\data\")  # <- edit if needed\n",
    "human_train_npy_path = data_root_path / \"train\" / \"train_human.npy\"\n",
    "ai_train_npy_path    = data_root_path / \"train\" / \"train_ai.npy\"\n",
    "validation_jsonl_path = data_root_path / \"val\" / \"validation.jsonl\"  # optional; not required here\n",
    "\n",
    "# Where to save artifacts (kept dims, whitening, weights, report)\n",
    "output_dir_path = Path(\".\")\n",
    "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------- reproducibility -----------------\n",
    "def set_global_seed(seed: int = 42, deterministic: bool = False):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ----------------- configuration -----------------\n",
    "training_config_dict = dict(\n",
    "    random_seed_integer = 42,\n",
    "    deterministic_cudnn_boolean = False,\n",
    "\n",
    "    # data\n",
    "    test_size_fraction = 0.15,\n",
    "\n",
    "    # model\n",
    "    input_embedding_dimensionality = 768,   # will be overwritten AFTER masking\n",
    "    sequence_length_tokens = 100,\n",
    "    model_hidden_dimensionality = 320,\n",
    "    transformer_num_layers = 2,\n",
    "    transformer_num_heads = 8,\n",
    "    transformer_ffn_multiplier = 4,\n",
    "    attention_dropout_rate = 0.10,\n",
    "    ffn_dropout_rate = 0.10,\n",
    "    use_rnn_boolean = True,\n",
    "    rnn_num_layers = 1,\n",
    "    use_cnn_boolean = True,\n",
    "    cnn_kernel_sizes_list = [3, 5, 7],\n",
    "    cnn_out_channels = 96,\n",
    "    use_multiscale_pooling_boolean = True,\n",
    "    use_checkpointing_boolean = True,\n",
    "\n",
    "    # head\n",
    "    head_type_string = \"cosine\",    # \"cosine\" (ArcFace-style) or \"linear\"\n",
    "    arcface_scale_float = 16.0,\n",
    "    arcface_margin_float = 0.20,\n",
    "    arcface_easy_margin_boolean = False,\n",
    "\n",
    "    # training\n",
    "    total_epochs_integer = 12,\n",
    "    batch_size_train = 64,\n",
    "    batch_size_eval = 128,\n",
    "    grad_accumulation_steps = 2,\n",
    "    base_learning_rate = 2e-4,\n",
    "    weight_decay_rate = 1e-2,\n",
    "    lr_warmup_fraction = 0.10,\n",
    "    max_grad_norm = 2.0,\n",
    "    early_stop_patience_epochs = 6,\n",
    "\n",
    "    # regularization & tricks\n",
    "    use_amp_boolean = True,\n",
    "    use_class_weights_boolean = True,\n",
    "    use_rdrop_alpha = 2e-2,      # 0 to disable\n",
    "    token_mix_probability = 0.15,\n",
    "\n",
    "    # evaluation\n",
    "    optimize_for_string = \"acc\",  # \"acc\" or \"f1\"\n",
    "    mc_dropout_samples = 1,       # >1 enables MC-dropout\n",
    "\n",
    "    # numerics\n",
    "    mask_topk_high_variance_dims = 64,  # 0 to disable\n",
    "    out_dir = str(output_dir_path)\n",
    ")\n",
    "\n",
    "device_torch = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_global_seed(training_config_dict[\"random_seed_integer\"], training_config_dict[\"deterministic_cudnn_boolean\"])\n",
    "\n",
    "# ----------------- numerical guards -----------------\n",
    "def safe_softmax_from_logits(logits_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    logits_tensor = logits_tensor.float()\n",
    "    logits_tensor = torch.nan_to_num(logits_tensor, nan=0.0, neginf=-50.0, posinf=50.0).clamp(-50.0, 50.0)\n",
    "    logits_tensor = logits_tensor - logits_tensor.max(dim=1, keepdim=True).values\n",
    "    prob_tensor = torch.softmax(logits_tensor, dim=1)\n",
    "    prob_tensor = torch.nan_to_num(prob_tensor, nan=0.5, neginf=0.0, posinf=1.0)\n",
    "    return prob_tensor\n",
    "\n",
    "def np_clean_probabilities(array_1d: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(array_1d, nan=0.5, neginf=0.0, posinf=1.0)\n",
    "\n",
    "def cosine_warmup_lr(optimizer_like, current_step: int, total_steps: int, base_lr: float, warmup_fraction: float):\n",
    "    warmup_steps = max(1, int(total_steps * warmup_fraction))\n",
    "    if current_step < warmup_steps:\n",
    "        lr_value = base_lr * (current_step + 1) / warmup_steps\n",
    "    else:\n",
    "        progress = (current_step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        lr_value = 0.5 * base_lr * (1.0 + math.cos(math.pi * progress))\n",
    "    for pg in optimizer_like.param_groups:\n",
    "        pg[\"lr\"] = lr_value\n",
    "\n",
    "def mixup_batch(inputs_tensor: torch.Tensor, targets_tensor: torch.Tensor, alpha: float = 0.3):\n",
    "    if alpha <= 0:\n",
    "        return inputs_tensor, targets_tensor, targets_tensor, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(inputs_tensor.size(0), device=inputs_tensor.device)\n",
    "    mixed_inputs = lam * inputs_tensor + (1 - lam) * inputs_tensor[idx]\n",
    "    targets_a, targets_b = targets_tensor, targets_tensor[idx]\n",
    "    return mixed_inputs, targets_a, targets_b, lam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72c766",
   "metadata": {},
   "source": [
    "#Cell 2 â€” Load data, preprocessing, split, and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c5a4e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready.\n",
      "Train: (13873, 100, 704) | Holdout: (2449, 100, 704) | Kept dims: (704,)\n",
      "Effective input dim = 704 | Whitening saved to: D:\\ML_CHALLANGE\\models\\models\n"
     ]
    }
   ],
   "source": [
    "# --- UltraHybrid XL++: Cell 2 / Data loading & preprocessing ---\n",
    "\n",
    "# 1) load arrays\n",
    "assert human_train_npy_path.exists() and ai_train_npy_path.exists(), \"NPY files not found.\"\n",
    "human_embeddings_np = np.load(human_train_npy_path)   # (Nh, 100, 768)\n",
    "ai_embeddings_np    = np.load(ai_train_npy_path)      # (Na, 100, 768)\n",
    "assert human_embeddings_np.ndim == 3 and ai_embeddings_np.ndim == 3, \"Bad shapes for input arrays.\"\n",
    "\n",
    "# 2) labels and split (stratified)\n",
    "labels_human = np.zeros(len(human_embeddings_np), dtype=np.int64)\n",
    "labels_ai    = np.ones(len(ai_embeddings_np), dtype=np.int64)\n",
    "all_embeddings_np = np.concatenate([human_embeddings_np, ai_embeddings_np], axis=0)\n",
    "all_labels_np     = np.concatenate([labels_human, labels_ai], axis=0)\n",
    "\n",
    "X_train_np, X_hold_np, y_train_np, y_hold_np = train_test_split(\n",
    "    all_embeddings_np, all_labels_np,\n",
    "    test_size=training_config_dict[\"test_size_fraction\"],\n",
    "    random_state=training_config_dict[\"random_seed_integer\"],\n",
    "    stratify=all_labels_np\n",
    ")\n",
    "\n",
    "# 3) per-sample layer-norm (stable)\n",
    "def layernorm_2d(sample_np: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    # sample_np: (T=100, D)\n",
    "    mean = sample_np.mean(axis=1, keepdims=True)\n",
    "    var  = sample_np.var(axis=1, keepdims=True)\n",
    "    return ((sample_np - mean) / np.sqrt(var + eps)).astype(np.float32)\n",
    "\n",
    "def preprocess_batch_seqnorm(X3d: np.ndarray) -> np.ndarray:\n",
    "    out = np.empty_like(X3d, dtype=np.float32)\n",
    "    for i in range(len(X3d)):\n",
    "        out[i] = layernorm_2d(X3d[i])\n",
    "    return out\n",
    "\n",
    "X_train_seq = preprocess_batch_seqnorm(X_train_np)\n",
    "X_hold_seq  = preprocess_batch_seqnorm(X_hold_np)\n",
    "\n",
    "# 4) mask top-K high-variance dimensions (computed on TRAIN only)\n",
    "def mask_noisiest_dimensions(X_train: np.ndarray, X_hold: np.ndarray, topk: int):\n",
    "    if topk <= 0:\n",
    "        keep_idx = np.arange(X_train.shape[-1])\n",
    "        return X_train, X_hold, keep_idx\n",
    "    flat = X_train.reshape(-1, X_train.shape[-1])  # (N*T, D)\n",
    "    var_per_dim = flat.var(axis=0)\n",
    "    keep_idx = np.argsort(var_per_dim)[:-topk]     # drop highest-variance dims\n",
    "    return X_train[..., keep_idx], X_hold[..., keep_idx], keep_idx\n",
    "\n",
    "X_train_seq_mask, X_hold_seq_mask, kept_dim_indices = mask_noisiest_dimensions(\n",
    "    X_train_seq, X_hold_seq, training_config_dict[\"mask_topk_high_variance_dims\"]\n",
    ")\n",
    "\n",
    "# 5) set the true input dim AFTER masking\n",
    "effective_input_dim = int(X_train_seq_mask.shape[-1])\n",
    "training_config_dict[\"input_embedding_dimensionality\"] = effective_input_dim\n",
    "\n",
    "# 6) compute & save whitening on the *masked* TRAIN set\n",
    "flat_train = X_train_seq_mask.reshape(-1, effective_input_dim)\n",
    "whiten_mu = flat_train.mean(axis=0).astype(np.float32)\n",
    "whiten_sd = np.clip(flat_train.std(axis=0), 1e-6, None).astype(np.float32)\n",
    "\n",
    "np.save(output_dir_path / \"kept_dim_indices.npy\", kept_dim_indices.astype(np.int32))\n",
    "np.save(output_dir_path / \"whiten_mu.npy\", whiten_mu)\n",
    "np.save(output_dir_path / \"whiten_sd.npy\", whiten_sd)\n",
    "\n",
    "# 7) Torch datasets/loaders\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_seq_mask), torch.from_numpy(y_train_np))\n",
    "hold_dataset  = TensorDataset(torch.from_numpy(X_hold_seq_mask),  torch.from_numpy(y_hold_np))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_config_dict[\"batch_size_train\"],\n",
    "                          shuffle=True, num_workers=0, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_dataset,  batch_size=training_config_dict[\"batch_size_eval\"],\n",
    "                          shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(\"Data ready.\")\n",
    "print(f\"Train: {X_train_seq_mask.shape} | Holdout: {X_hold_seq_mask.shape} | Kept dims: {kept_dim_indices.shape}\")\n",
    "print(f\"Effective input dim = {effective_input_dim} | Whitening saved to: {output_dir_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180e063",
   "metadata": {},
   "source": [
    "#Cell 3 â€” Model blocks (Transformer, ArcFace head, unified model forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UltraHybrid XL++: Cell 3 / Model definition ---\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint as ckpt\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, ffn_dim: int,\n",
    "                 gated_attention: bool = True, attn_dropout: float = 0.1, ffn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.gated_attention = gated_attention\n",
    "\n",
    "        self.Wq = nn.Linear(dim, dim)\n",
    "        self.Wk = nn.Linear(dim, dim)\n",
    "        self.Wv = nn.Linear(dim, dim)\n",
    "        self.Wo = nn.Linear(dim, dim)\n",
    "        self.gate_proj = nn.Linear(dim, 1) if gated_attention else None\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, ffn_dim), nn.GELU(), nn.Dropout(ffn_dropout),\n",
    "            nn.Linear(ffn_dim, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.drop_attn = nn.Dropout(attn_dropout)\n",
    "        self.drop_ffn  = nn.Dropout(ffn_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, D = x.shape\n",
    "        q = self.Wq(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.Wk(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.Wv(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if self.gated_attention:\n",
    "            gate = torch.sigmoid(self.gate_proj(x)).unsqueeze(1)  # (B,1,L,1)\n",
    "            k = k * gate\n",
    "            v = v * gate\n",
    "\n",
    "        scores = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        weights = self.drop_attn(torch.softmax(scores, dim=-1))\n",
    "        attn_out = (weights @ v).transpose(1, 2).reshape(B, L, D)\n",
    "\n",
    "        x = self.norm1(x + self.Wo(attn_out))\n",
    "        z = self.drop_ffn(self.ffn(x))\n",
    "        return self.norm2(x + z)\n",
    "\n",
    "class CosineMarginHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, scale: float = 16.0, margin: float = 0.20, easy_margin: bool = False):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.easy = easy_margin\n",
    "        self.weight_matrix = nn.Parameter(nn.init.orthogonal_(torch.empty(2, in_dim)))\n",
    "\n",
    "    def forward(self, feature_vectors, labels: Optional[torch.Tensor] = None):\n",
    "        features_norm = F.normalize(feature_vectors, dim=1)\n",
    "        weights_norm  = F.normalize(self.weight_matrix, dim=1)\n",
    "        cosine_vals = features_norm @ weights_norm.t()              # (B,2)\n",
    "\n",
    "        if labels is None:\n",
    "            return self.scale * cosine_vals\n",
    "\n",
    "        # apply additive margin only on the target class\n",
    "        clamped = cosine_vals.clamp(-1 + 1e-7, 1 - 1e-7)\n",
    "        target = clamped[torch.arange(len(feature_vectors)), labels]\n",
    "        target_m = torch.where(target > 0, target - self.margin, target) if self.easy else (target - self.margin)\n",
    "        cosine_with_margin = cosine_vals.clone()\n",
    "        cosine_with_margin[torch.arange(len(feature_vectors)), labels] = target_m\n",
    "        return self.scale * cosine_with_margin\n",
    "\n",
    "class UltraHybridModel(nn.Module):\n",
    "    def __init__(self, cfg: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = dict(cfg)\n",
    "        d_in = int(self.cfg[\"input_embedding_dimensionality\"])\n",
    "        d    = int(self.cfg[\"model_hidden_dimensionality\"])\n",
    "        ffn_dim = int(self.cfg[\"transformer_ffn_multiplier\"] * d)\n",
    "\n",
    "        # input projection (if masked dim != model dim)\n",
    "        self.input_projection = nn.Linear(d_in, d) if d_in != d else None\n",
    "\n",
    "        # transformer stack\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d, self.cfg[\"transformer_num_heads\"], ffn_dim,\n",
    "                             gated_attention=True,\n",
    "                             attn_dropout=self.cfg[\"attention_dropout_rate\"],\n",
    "                             ffn_dropout=self.cfg[\"ffn_dropout_rate\"])\n",
    "            for _ in range(self.cfg[\"transformer_num_layers\"])\n",
    "        ])\n",
    "\n",
    "        # optional heads\n",
    "        self.use_rnn = bool(self.cfg[\"use_rnn_boolean\"])\n",
    "        if self.use_rnn:\n",
    "            self.rnn = nn.GRU(input_size=d, hidden_size=d, num_layers=self.cfg[\"rnn_num_layers\"],\n",
    "                              batch_first=True, bidirectional=True, dropout=0.0)\n",
    "            self.rnn_out_dim = 2 * d\n",
    "        else:\n",
    "            self.rnn_out_dim = 0\n",
    "\n",
    "        self.use_cnn = bool(self.cfg[\"use_cnn_boolean\"])\n",
    "        if self.use_cnn:\n",
    "            ch = self.cfg[\"cnn_out_channels\"]; ks = self.cfg[\"cnn_kernel_sizes_list\"]\n",
    "            self.conv_layers = nn.ModuleList([nn.Conv1d(d, ch, k, padding=(k-1)//2) for k in ks])\n",
    "            self.cnn_out_dim = len(ks) * (2 * ch if self.cfg[\"use_multiscale_pooling_boolean\"] else ch)\n",
    "        else:\n",
    "            self.cnn_out_dim = 0\n",
    "\n",
    "        self.use_multiscale_pool = bool(self.cfg[\"use_multiscale_pooling_boolean\"])\n",
    "        transformer_out_dim = 2 * d if self.use_multiscale_pool else d\n",
    "\n",
    "        # minimal engineered extras\n",
    "        self.extra_dim = 3  # mean L2, std L2, mean cos to seq centroid\n",
    "\n",
    "        total_feature_dim = transformer_out_dim + self.rnn_out_dim + self.cnn_out_dim + self.extra_dim\n",
    "\n",
    "        head_type = self.cfg.get(\"head_type_string\", \"linear\")\n",
    "        if head_type == \"cosine\":\n",
    "            self.classifier_head = CosineMarginHead(total_feature_dim,\n",
    "                                                    self.cfg[\"arcface_scale_float\"],\n",
    "                                                    self.cfg[\"arcface_margin_float\"],\n",
    "                                                    self.cfg[\"arcface_easy_margin_boolean\"])\n",
    "        else:\n",
    "            self.classifier_head = nn.Linear(total_feature_dim, 2)\n",
    "\n",
    "        # whitening buffers (load if present, else neutral) â€” dimension-safe\n",
    "        mu_path = Path(self.cfg[\"out_dir\"]) / \"whiten_mu.npy\"\n",
    "        sd_path = Path(self.cfg[\"out_dir\"]) / \"whiten_sd.npy\"\n",
    "        if mu_path.exists() and sd_path.exists():\n",
    "            mu = np.load(mu_path).astype(np.float32)\n",
    "            sd = np.load(sd_path).astype(np.float32)\n",
    "            if mu.shape[0] == d_in and sd.shape[0] == d_in:\n",
    "                self.register_buffer(\"whiten_mu\", torch.from_numpy(mu))\n",
    "                self.register_buffer(\"whiten_sd\", torch.from_numpy(sd))\n",
    "            else:\n",
    "                # fallback if stale files exist\n",
    "                self.register_buffer(\"whiten_mu\", torch.zeros(d_in))\n",
    "                self.register_buffer(\"whiten_sd\", torch.ones(d_in))\n",
    "        else:\n",
    "            self.register_buffer(\"whiten_mu\", torch.zeros(d_in))\n",
    "            self.register_buffer(\"whiten_sd\", torch.ones(d_in))\n",
    "\n",
    "        self._warned_whiten_shape = False  # one-time warning\n",
    "\n",
    "    # ----- engineered features -----\n",
    "    @staticmethod\n",
    "    def token_l2_mean_std(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        norms = torch.linalg.vector_norm(x, dim=2)  # (B,T)\n",
    "        return norms.mean(1, keepdim=True), norms.std(1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_cosine_to_seq_centroid(x: torch.Tensor) -> torch.Tensor:\n",
    "        mean_vec = x.mean(1)\n",
    "        mean_vec = mean_vec / (mean_vec.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        x_normed = x / (x.norm(dim=2, keepdim=True) + 1e-9)\n",
    "        return ((x_normed * mean_vec.unsqueeze(1)).sum(2)).mean(1, keepdim=True)\n",
    "\n",
    "    def time_weighted_mean(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        T = z.size(1)\n",
    "        w = torch.linspace(1.3, 0.7, T, device=z.device, dtype=z.dtype).view(1, T, 1)\n",
    "        return (z * w).sum(1) / (w.sum().clamp_min(1e-8))\n",
    "\n",
    "    # ----- robust whitening -----\n",
    "    def _apply_whitening(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = x.shape\n",
    "        if self.whiten_mu.numel() != D or self.whiten_sd.numel() != D:\n",
    "            if not self._warned_whiten_shape:\n",
    "                print(f\"[warn] Whitening dim mismatch: x:{D} vs buffers:{self.whiten_mu.numel()}. Using batch stats.\")\n",
    "                self._warned_whiten_shape = True\n",
    "            flat = x.reshape(-1, D)\n",
    "            mu = flat.mean(dim=0)\n",
    "            sd = flat.std(dim=0).clamp_min(1e-6)\n",
    "            return (x - mu.view(1,1,-1)) / sd.view(1,1,-1)\n",
    "        return (x - self.whiten_mu.view(1,1,-1)) / (self.whiten_sd.view(1,1,-1) + 1e-6)\n",
    "\n",
    "    # ----- unified forward -----\n",
    "    def forward(self, token_embeddings: torch.Tensor,\n",
    "                return_features: bool = False,\n",
    "                labels: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        assert token_embeddings.dim() == 3, \"Expect [batch, seq, dim]\"\n",
    "        x = self._apply_whitening(token_embeddings)\n",
    "\n",
    "        xp = self.input_projection(x) if self.input_projection is not None else x\n",
    "\n",
    "        z = xp\n",
    "        for blk in self.transformer_blocks:\n",
    "            z = ckpt(blk, z) if (self.cfg[\"use_checkpointing_boolean\"] and self.training) else blk(z)\n",
    "\n",
    "        transformer_feat = (torch.cat([self.time_weighted_mean(z), z.max(1).values], dim=1)\n",
    "                            if self.use_multiscale_pool else self.time_weighted_mean(z))\n",
    "\n",
    "        rnn_feat = None\n",
    "        if self.use_rnn:\n",
    "            _, h = self.rnn(xp)\n",
    "            h_fwd = h[-2] if self.cfg[\"rnn_num_layers\"] >= 1 else h[0]\n",
    "            h_bwd = h[-1] if self.cfg[\"rnn_num_layers\"] >= 1 else h[0]\n",
    "            rnn_feat = torch.cat([h_fwd, h_bwd], dim=1)\n",
    "\n",
    "        cnn_feat = None\n",
    "        if self.use_cnn:\n",
    "            xc = xp.transpose(1, 2)\n",
    "            feats = []\n",
    "            for conv in self.conv_layers:\n",
    "                y = F.relu(conv(xc))\n",
    "                if self.use_multiscale_pool:\n",
    "                    feats.extend([y.max(2).values, y.mean(2)])\n",
    "                else:\n",
    "                    feats.append(y.max(2).values)\n",
    "            cnn_feat = torch.cat(feats, dim=1)\n",
    "\n",
    "        mean_l2, std_l2 = self.token_l2_mean_std(x)\n",
    "        mean_cos = self.mean_cosine_to_seq_centroid(x)\n",
    "        extra = torch.cat([mean_l2, std_l2, mean_cos], dim=1)\n",
    "\n",
    "        parts = [transformer_feat, extra]\n",
    "        if rnn_feat is not None: parts.append(rnn_feat)\n",
    "        if cnn_feat is not None: parts.append(cnn_feat)\n",
    "        features = torch.cat(parts, dim=1) if len(parts) > 1 else parts[0]\n",
    "\n",
    "        if return_features:\n",
    "            return features\n",
    "\n",
    "        if self.cfg[\"head_type_string\"] == \"cosine\":\n",
    "            return self.classifier_head(features, labels=labels)\n",
    "        else:\n",
    "            return self.classifier_head(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b9e8a",
   "metadata": {},
   "source": [
    "#Cell 4 â€” Training/eval utilities (metrics, evaluation loop with safe softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f36bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UltraHybrid XL++: Cell 4 / Metrics & Eval ---\n",
    "\n",
    "def tune_threshold(y_true: np.ndarray, p_pos: np.ndarray, mode: str = \"acc\") -> float:\n",
    "    grid = np.linspace(0.0, 1.0, 501)\n",
    "    if mode == \"acc\":\n",
    "        best_t, best_s = 0.5, -1.0\n",
    "        for t in grid:\n",
    "            s = (y_true == (p_pos >= t).astype(int)).mean()\n",
    "            if s > best_s: best_t, best_s = t, s\n",
    "        return float(best_t)\n",
    "    # mode == \"f1\"\n",
    "    best_t, best_f1, best_j = 0.5, -1.0, -1.0\n",
    "    for t in grid:\n",
    "        pred = (p_pos >= t).astype(int)\n",
    "        p = precision_score(y_true, pred, zero_division=0)\n",
    "        r = recall_score(y_true, pred, zero_division=0)\n",
    "        f1 = 2*p*r/(p+r) if (p+r) > 0 else 0.0\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0,1]).ravel()\n",
    "        j = (tp/(tp+fn+1e-9)) - (fp/(fp+tn+1e-9))\n",
    "        if (f1 > best_f1) or (abs(f1-best_f1) < 1e-9 and j > best_j):\n",
    "            best_t, best_f1, best_j = t, f1, j\n",
    "    return float(best_t)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_probabilities(model: nn.Module, data_loader: DataLoader) -> np.ndarray:\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    # force float32 eval for stability, even on CUDA\n",
    "    ctx = torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=False) if torch.cuda.is_available() else torch.cpu.amp.autocast(enabled=False)\n",
    "    with ctx:\n",
    "        for xb, _ in data_loader:\n",
    "            xb = xb.to(device_torch, non_blocking=True)\n",
    "            logits = model(xb)  # labels=None â†’ eval path\n",
    "            probs = safe_softmax_from_logits(logits)[:, 1]\n",
    "            probs_list.append(probs.cpu().numpy())\n",
    "    probs = np.concatenate(probs_list)\n",
    "    return np_clean_probabilities(probs)\n",
    "\n",
    "def print_classification_report(y_true: np.ndarray, p_pos: np.ndarray, tag: str):\n",
    "    thr = tune_threshold(y_true, p_pos, training_config_dict[\"optimize_for_string\"])\n",
    "    y_pred = (p_pos >= thr).astype(int)\n",
    "    auc = roc_auc_score(y_true, p_pos)\n",
    "    pr  = average_precision_score(y_true, p_pos)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    print(f\"\\n[{tag}] AUC={auc:.4f}  PR-AUC={pr:.4f}  F1@thr={f1:.4f}  ACC@thr={acc:.4f}  Thr={thr:.3f}\")\n",
    "    print(f\"        CM [[{tn:4d} {fp:4d}]\\n            [{fn:4d} {tp:4d}]]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81383e",
   "metadata": {},
   "source": [
    "#Cell 5 â€” Train loop (unified forward, stable AMP, clear prints, saving best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7011680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] avg_loss=2.1514 | holdout AUC=0.92340\n",
      "[Epoch 02] avg_loss=1.2664 | holdout AUC=0.95212\n",
      "[Epoch 03] avg_loss=1.2074 | holdout AUC=0.95699\n",
      "[Epoch 04] avg_loss=1.1040 | holdout AUC=0.95530\n",
      "[Epoch 05] avg_loss=1.1105 | holdout AUC=0.95826\n",
      "[Epoch 06] avg_loss=1.0424 | holdout AUC=0.96061\n",
      "[Epoch 07] avg_loss=1.1109 | holdout AUC=0.96003\n",
      "[Epoch 08] avg_loss=1.0761 | holdout AUC=0.95744\n",
      "[Epoch 09] avg_loss=0.9249 | holdout AUC=0.96130\n",
      "[Epoch 10] avg_loss=1.0502 | holdout AUC=0.96205\n",
      "[Epoch 11] avg_loss=0.9938 | holdout AUC=0.96249\n",
      "[Epoch 12] avg_loss=0.8428 | holdout AUC=0.96244\n",
      "Best epoch: 11 | Best holdout AUC: 0.96249\n",
      "\n",
      "[HOLDOUT-FINAL] AUC=0.9625  PR-AUC=0.9610  F1@thr=0.9039  ACC@thr=0.9008  Thr=0.108\n",
      "        CM [[1063  162]\n",
      "            [  81 1143]]\n",
      "Saved: ultrahybrid_xlpp_best.pt, holdout_probabilities.npy, training_report.json, kept_dim_indices.npy, whiten_mu.npy, whiten_sd.npy\n"
     ]
    }
   ],
   "source": [
    "# --- UltraHybrid XL++: Cell 5 / Training loop ---\n",
    "\n",
    "# Build model AFTER Cell 2 set input dim & saved whitening\n",
    "training_config_dict[\"out_dir\"] = str(output_dir_path)\n",
    "model_instance = UltraHybridModel(training_config_dict).to(device_torch)\n",
    "\n",
    "# class weights (optional)\n",
    "if training_config_dict[\"use_class_weights_boolean\"]:\n",
    "    n0 = int((y_train_np == 0).sum()); n1 = int((y_train_np == 1).sum())\n",
    "    inv = np.array([1.0/max(1,n0), 1.0/max(1,n1)], dtype=np.float32)\n",
    "    class_weights_tensor = torch.tensor(inv / inv.mean(), device=device_torch)\n",
    "else:\n",
    "    class_weights_tensor = None\n",
    "\n",
    "criterion_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer_torch = torch.optim.AdamW(\n",
    "    model_instance.parameters(),\n",
    "    lr=training_config_dict[\"base_learning_rate\"],\n",
    "    weight_decay=training_config_dict[\"weight_decay_rate\"]\n",
    ")\n",
    "scaler_amp = torch.cuda.amp.GradScaler(enabled=(training_config_dict[\"use_amp_boolean\"] and device_torch.type==\"cuda\"))\n",
    "\n",
    "total_update_steps = training_config_dict[\"total_epochs_integer\"] * max(1, math.ceil(len(train_loader)/max(1, training_config_dict[\"grad_accumulation_steps\"])))\n",
    "global_step_counter = 0\n",
    "\n",
    "best_holdout_auc = -1.0\n",
    "best_state_dict = None\n",
    "best_epoch_index = -1\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch_index in range(1, training_config_dict[\"total_epochs_integer\"] + 1):\n",
    "    model_instance.train()\n",
    "    running_loss_sum = 0.0\n",
    "    step_in_epoch = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device_torch, non_blocking=True)\n",
    "        yb = yb.to(device_torch, non_blocking=True)\n",
    "\n",
    "        # optional token-wise mixup (disabled by default for sequences; left here for completeness)\n",
    "        if training_config_dict[\"token_mix_probability\"] > 0.0 and np.random.rand() < training_config_dict[\"token_mix_probability\"]:\n",
    "            xb, ya, yb2, lam = mixup_batch(xb, yb, alpha=0.3)\n",
    "            use_mixup = True\n",
    "        else:\n",
    "            use_mixup = False\n",
    "\n",
    "        cosine_warmup_lr(optimizer_torch, global_step_counter, total_update_steps,\n",
    "                         training_config_dict[\"base_learning_rate\"], training_config_dict[\"lr_warmup_fraction\"])\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(training_config_dict[\"use_amp_boolean\"] and device_torch.type==\"cuda\")):\n",
    "            logits = model_instance(xb, labels=yb)  # unified: labelsâ†’ArcFace margin used internally\n",
    "\n",
    "            if training_config_dict[\"use_rdrop_alpha\"] > 0.0:\n",
    "                logits2 = model_instance(xb, labels=yb)\n",
    "                ce1 = criterion_fn(logits,  yb)\n",
    "                ce2 = criterion_fn(logits2, yb)\n",
    "                p1 = F.log_softmax(logits,  dim=1); p2 = F.log_softmax(logits2, dim=1)\n",
    "                kl = 0.5 * (F.kl_div(p1, p2.exp(), reduction=\"batchmean\") + F.kl_div(p2, p1.exp(), reduction=\"batchmean\"))\n",
    "                loss = 0.5*(ce1+ce2) + training_config_dict[\"use_rdrop_alpha\"]*kl\n",
    "            else:\n",
    "                if use_mixup:\n",
    "                    loss = lam * criterion_fn(logits, ya) + (1 - lam) * criterion_fn(logits, yb2)\n",
    "                else:\n",
    "                    loss = criterion_fn(logits, yb)\n",
    "\n",
    "            loss = loss / training_config_dict[\"grad_accumulation_steps\"]\n",
    "\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        step_in_epoch += 1\n",
    "\n",
    "        if (step_in_epoch % training_config_dict[\"grad_accumulation_steps\"]) == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model_instance.parameters(), training_config_dict[\"max_grad_norm\"])\n",
    "            scaler_amp.step(optimizer_torch)\n",
    "            scaler_amp.update()\n",
    "            optimizer_torch.zero_grad(set_to_none=True)\n",
    "            global_step_counter += 1\n",
    "\n",
    "        running_loss_sum += loss.item() * training_config_dict[\"grad_accumulation_steps\"]\n",
    "\n",
    "    # ----- epoch end: evaluate -----\n",
    "    hold_probs = evaluate_model_probabilities(model_instance, hold_loader)\n",
    "    hold_auc = roc_auc_score(y_hold_np, hold_probs)\n",
    "\n",
    "    print(f\"[Epoch {epoch_index:02d}] avg_loss={running_loss_sum/max(1, step_in_epoch):.4f} | holdout AUC={hold_auc:.5f}\")\n",
    "\n",
    "    if hold_auc > best_holdout_auc + 1e-4:\n",
    "        best_holdout_auc = hold_auc\n",
    "        best_state_dict = copy.deepcopy(model_instance.state_dict())\n",
    "        best_epoch_index = epoch_index\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= training_config_dict[\"early_stop_patience_epochs\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# restore best\n",
    "if best_state_dict is not None:\n",
    "    model_instance.load_state_dict(best_state_dict)\n",
    "\n",
    "print(f\"Best epoch: {best_epoch_index} | Best holdout AUC: {best_holdout_auc:.5f}\")\n",
    "\n",
    "# final report on holdout (thresholded metrics)\n",
    "final_hold_probs = evaluate_model_probabilities(model_instance, hold_loader)\n",
    "print_classification_report(y_hold_np, final_hold_probs, tag=\"HOLDOUT-FINAL\")\n",
    "\n",
    "# save artifacts\n",
    "torch.save(model_instance.state_dict(), output_dir_path / \"ultrahybrid_xlpp_best.pt\")\n",
    "np.save(output_dir_path / \"holdout_probabilities.npy\", final_hold_probs)\n",
    "with open(output_dir_path / \"training_report.json\", \"w\") as f:\n",
    "    json.dump(dict(\n",
    "        best_epoch=int(best_epoch_index),\n",
    "        best_holdout_auc=float(best_holdout_auc),\n",
    "        kept_dimensions=kept_dim_indices.tolist(),\n",
    "        effective_input_dim=effective_input_dim,\n",
    "        config=training_config_dict\n",
    "    ), f, indent=2)\n",
    "print(\"Saved: ultrahybrid_xlpp_best.pt, holdout_probabilities.npy, training_report.json, kept_dim_indices.npy, whiten_mu.npy, whiten_sd.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
