{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cded74-0c72-44f6-b515-da50a2b89735",
   "metadata": {},
   "source": [
    "### Paths & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7ff9e5-83b7-480c-9c06-7c75ea3fbfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: D:\\Adelaide\\2ndTrimester2 2025\\Using Machine Learning Tools_4536_COMP_SCI_X_0001\\A3\n",
      "Files: ['achala.ipynb', 'ALFIE_pre_processing.ipynb', 'best_meta.json', 'best_name.txt', 'best_sgd_stream.joblib', 'best_sklearn.joblib', 'kaggle_language_detection_advanced.ipynb', 'sample.csv', 'Sanjida_Amrin_a1934493.ipynb', 'test_features.jsonl'] ...\n"
     ]
    }
   ],
   "source": [
    "# Paths & config\n",
    "from pathlib import Path\n",
    "\n",
    "CANDIDATES = [\n",
    "    Path(r\"D:\\Adelaide\\2ndTrimester2 2025\\Using Machine Learning Tools_4536_COMP_SCI_X_0001\\A3\"),\n",
    "    Path.cwd()\n",
    "]\n",
    "A3_DIR = next((p for p in CANDIDATES if (p / \"train_ai.npy\").exists()), Path.cwd())\n",
    "\n",
    "DATA_DIR   = A3_DIR\n",
    "SUBMIT_DIR = A3_DIR / \"submissions\"\n",
    "SUBMIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Memory / speed knobs\n",
    "BATCH_TRAIN = 24   # streaming training for batch size (lower if RAM is tight)\n",
    "BATCH_TEST  = 256  # streaming test prediction for batch size\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Files:\", [p.name for p in DATA_DIR.iterdir() if p.is_file()][:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ce1f0-c30e-4b2b-b4b8-403324f8b376",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8e3dd8-224a-40a6-89e5-74bb96d99f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: JSONL resolving, pooling, metrics, etc.\n",
    "import json, gc, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def resolve_jsonl(p: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Handling the cases where a *folder* is named 'validation.jsonl' or 'test_features.jsonl'.\n",
    "    And returns a file path to the actual .jsonl file.\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    if p.is_file():\n",
    "        return p\n",
    "    if p.is_dir():\n",
    "        cands = list(p.glob(\"*.jsonl\")) or list(p.rglob(\"*.jsonl\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"No .jsonl found in {p}\")\n",
    "        # Prefer expected filenames if present\n",
    "        for expected in (\"validation.jsonl\", \"test_features.jsonl\"):\n",
    "            for c in cands:\n",
    "                if c.name.lower() == expected:\n",
    "                    print(f\"Resolved {p} -> {c}\")\n",
    "                    return c\n",
    "        print(f\"Resolved {p} -> {cands[0]}\")\n",
    "        return cands[0]\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "def pool_stats_batch(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input:  x of shape (B, L, 768)\n",
    "    Output: (B, 3072) by concatenating [mean, std, max, min] along the time axis L.\n",
    "    \"\"\"\n",
    "    m  = x.mean(axis=1)\n",
    "    s  = x.std(axis=1)\n",
    "    mx = x.max(axis=1)\n",
    "    mn = x.min(axis=1)\n",
    "    return np.concatenate([m, s, mx, mn], axis=1).astype(np.float32)\n",
    "\n",
    "def pool_stats_flex(sample) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flexible pooling for any (L,768) or (768,) or k*768 shaped data -> (3072,) features\n",
    "    by concatenating [mean, std, max, min] across the sequence axis.\n",
    "    \"\"\"\n",
    "    x = np.asarray(sample, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        if x.size == 768:\n",
    "            m = x; s = np.zeros_like(x); mx = x; mn = x\n",
    "            return np.concatenate([m, s, mx, mn]).astype(np.float32)\n",
    "        if x.size % 768 == 0:\n",
    "            x = x.reshape(-1, 768)\n",
    "        else:\n",
    "            vec = (np.pad(x, (0, 768 - x.size)) if x.size < 768 else x[:768])\n",
    "            m = vec; s = np.zeros_like(vec); mx = vec; mn = vec\n",
    "            return np.concatenate([m, s, mx, mn]).astype(np.float32)\n",
    "    if x.ndim == 2:\n",
    "        if x.shape[1] == 768:\n",
    "            pass\n",
    "        elif x.shape[0] == 768:\n",
    "            x = x.T\n",
    "        elif (x.size % 768) == 0:\n",
    "            x = x.reshape(-1, 768)\n",
    "        else:\n",
    "            x = x[:, :768] if x.shape[1] > 768 else np.pad(x, ((0,0),(0,768-x.shape[1])), 'constant')\n",
    "    else:\n",
    "        if (x.size % 768) == 0:\n",
    "            x = x.reshape(-1, 768)\n",
    "        else:\n",
    "            vec = (np.pad(x.ravel(), (0, 768 - x.size)) if x.size < 768 else x.ravel()[:768])\n",
    "            m = vec; s = np.zeros_like(vec); mx = vec; mn = vec\n",
    "            return np.concatenate([m, s, mx, mn]).astype(np.float32)\n",
    "    m, s, mx, mn = x.mean(0), x.std(0), x.max(0), x.min(0)\n",
    "    return np.concatenate([m, s, mx, mn]).astype(np.float32)\n",
    "\n",
    "def n_rows_in_npy(npy_path: Path) -> int:\n",
    "    \"\"\"Return number of items in a .npy array (supports memmap).\"\"\"\n",
    "    arr = np.load(npy_path, mmap_mode=\"r\")\n",
    "    n = arr.shape[0] if hasattr(arr, \"shape\") else len(arr)\n",
    "    del arr\n",
    "    return int(n)\n",
    "\n",
    "def expit_stable(z):\n",
    "    \"\"\"Numerically stable sigmoid that avoids overflow for large |z|.\"\"\"\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    out = np.empty_like(z)\n",
    "    pos = z >= 0\n",
    "    out[pos]  = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    ez = np.exp(z[~pos])\n",
    "    out[~pos] = ez / (1.0 + ez)\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def optimal_threshold(y_true, proba):\n",
    "    \"\"\"\n",
    "    Return threshold with best F1 on y_true, plus F1 and AUC.\n",
    "    \"\"\"\n",
    "    ts = np.linspace(0.05, 0.95, 19)\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in ts:\n",
    "        f1 = f1_score(y_true, (proba >= t).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    auc = roc_auc_score(y_true, proba)\n",
    "    return best_t, best_f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d5f9c-617b-4d5d-b879-3bc09e3eff7d",
   "metadata": {},
   "source": [
    "### Training, Validating & Saving (streaming SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a211929d-4a44-4953-9fd0-70e462d16b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights -> w0=1.0000, w1=1.0000  (n0=8161, n1=8161)\n",
      "[scaler] train_ai.npy: 24/8161\n",
      "[scaler] train_ai.npy: 984/8161\n",
      "[scaler] train_ai.npy: 1944/8161\n",
      "[scaler] train_ai.npy: 2904/8161\n",
      "[scaler] train_ai.npy: 3864/8161\n",
      "[scaler] train_ai.npy: 4824/8161\n",
      "[scaler] train_ai.npy: 5784/8161\n",
      "[scaler] train_ai.npy: 6744/8161\n",
      "[scaler] train_ai.npy: 7704/8161\n",
      "[scaler] train_human.npy: 24/8161\n",
      "[scaler] train_human.npy: 984/8161\n",
      "[scaler] train_human.npy: 1944/8161\n",
      "[scaler] train_human.npy: 2904/8161\n",
      "[scaler] train_human.npy: 3864/8161\n",
      "[scaler] train_human.npy: 4824/8161\n",
      "[scaler] train_human.npy: 5784/8161\n",
      "[scaler] train_human.npy: 6744/8161\n",
      "[scaler] train_human.npy: 7704/8161\n",
      "[sgd] train_ai.npy: 24/8161\n",
      "[sgd] train_ai.npy: 984/8161\n",
      "[sgd] train_ai.npy: 1944/8161\n",
      "[sgd] train_ai.npy: 2904/8161\n",
      "[sgd] train_ai.npy: 3864/8161\n",
      "[sgd] train_ai.npy: 4824/8161\n",
      "[sgd] train_ai.npy: 5784/8161\n",
      "[sgd] train_ai.npy: 6744/8161\n",
      "[sgd] train_ai.npy: 7704/8161\n",
      "[sgd] train_human.npy: 24/8161\n",
      "[sgd] train_human.npy: 984/8161\n",
      "[sgd] train_human.npy: 1944/8161\n",
      "[sgd] train_human.npy: 2904/8161\n",
      "[sgd] train_human.npy: 3864/8161\n",
      "[sgd] train_human.npy: 4824/8161\n",
      "[sgd] train_human.npy: 5784/8161\n",
      "[sgd] train_human.npy: 6744/8161\n",
      "[sgd] train_human.npy: 7704/8161\n",
      "Validation: AUC=0.7850  F1@best=0.7778  thr=0.05\n",
      "Saved: D:\\Adelaide\\2ndTrimester2 2025\\Using Machine Learning Tools_4536_COMP_SCI_X_0001\\A3\\best_sgd_stream.joblib\n"
     ]
    }
   ],
   "source": [
    "# Streaming StandardScaler + SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import joblib\n",
    "\n",
    "ai_path = DATA_DIR / \"train_ai.npy\"\n",
    "hu_path = DATA_DIR / \"train_human.npy\"\n",
    "\n",
    "# For balanced learning via sample_weight class weights\n",
    "n_pos = n_rows_in_npy(ai_path)\n",
    "n_neg = n_rows_in_npy(hu_path)\n",
    "total = n_pos + n_neg\n",
    "w1 = total / (2.0 * n_pos)  # weight for class 1 (AI)\n",
    "w0 = total / (2.0 * n_neg)  # weight for class 0 (Human)\n",
    "print(f\"class weights -> w0={w0:.4f}, w1={w1:.4f}  (n0={n_neg}, n1={n_pos})\")\n",
    "\n",
    "# In streaming mode fit StandardScaler \n",
    "scaler = StandardScaler()\n",
    "for path in [ai_path, hu_path]:\n",
    "    arr = np.load(path, mmap_mode=\"r\")\n",
    "    if arr.ndim == 3:  # (N, 100, 768)\n",
    "        n = arr.shape[0]\n",
    "        for start in range(0, n, BATCH_TRAIN):\n",
    "            end = min(start + BATCH_TRAIN, n)\n",
    "            Xb = pool_stats_batch(np.asarray(arr[start:end]))\n",
    "            scaler.partial_fit(Xb)\n",
    "            if start % (BATCH_TRAIN*40) == 0:\n",
    "                print(f\"[scaler] {path.name}: {end}/{n}\")\n",
    "    else:  # object array\n",
    "        for i in range(len(arr)):\n",
    "            Xb = pool_stats_flex(arr[i])[None, :]\n",
    "            scaler.partial_fit(Xb)\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"[scaler] {path.name}: {i}/{len(arr)}\")\n",
    "    del arr; gc.collect()\n",
    "\n",
    "# Training SGDClassifier (logistic) with partial_fit + per-sample weights\n",
    "clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=1e-4,\n",
    "                    max_iter=1, tol=None, random_state=42)\n",
    "first = True\n",
    "classes = np.array([0, 1], dtype=int)\n",
    "\n",
    "for path, label in [(ai_path, 1), (hu_path, 0)]:\n",
    "    arr = np.load(path, mmap_mode=\"r\")\n",
    "    if arr.ndim == 3:\n",
    "        n = arr.shape[0]\n",
    "        for start in range(0, n, BATCH_TRAIN):\n",
    "            end = min(start + BATCH_TRAIN, n)\n",
    "            Xb = pool_stats_batch(np.asarray(arr[start:end]))\n",
    "            Xb = scaler.transform(Xb).astype(np.float32, copy=False)\n",
    "            yb = np.full(len(Xb), label, dtype=int)\n",
    "            sw = np.where(yb == 1, w1, w0).astype(np.float32)  # per-sample weights\n",
    "            if first:\n",
    "                clf.partial_fit(Xb, yb, classes=classes, sample_weight=sw); first = False\n",
    "            else:\n",
    "                clf.partial_fit(Xb, yb, sample_weight=sw)\n",
    "            if start % (BATCH_TRAIN*40) == 0:\n",
    "                print(f\"[sgd] {path.name}: {end}/{n}\")\n",
    "    else:\n",
    "        for i in range(len(arr)):\n",
    "            Xb = pool_stats_flex(arr[i])[None, :]\n",
    "            Xb = scaler.transform(Xb).astype(np.float32, copy=False)\n",
    "            yb = np.array([label], dtype=int)\n",
    "            sw = np.array([w1 if label == 1 else w0], dtype=np.float32)\n",
    "            if first:\n",
    "                clf.partial_fit(Xb, yb, classes=classes, sample_weight=sw); first = False\n",
    "            else:\n",
    "                clf.partial_fit(Xb, yb, sample_weight=sw)\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"[sgd] {path.name}: {i}/{len(arr)}\")\n",
    "    del arr; gc.collect()\n",
    "\n",
    "# Validation (As a tiny set: treating metrics cautiously)\n",
    "VAL_PATH = resolve_jsonl(DATA_DIR / \"validation.jsonl\")\n",
    "vitems = read_jsonl(VAL_PATH)\n",
    "y_val  = np.array([it[\"label\"] for it in vitems], dtype=int)\n",
    "X_val  = np.vstack([pool_stats_flex(it[\"features\"]) for it in vitems]).astype(np.float32)\n",
    "X_val  = scaler.transform(X_val).astype(np.float32, copy=False)\n",
    "\n",
    "scores_val = clf.decision_function(X_val)\n",
    "proba_val  = expit_stable(scores_val)\n",
    "thr, f1, auc = optimal_threshold(y_val, proba_val)\n",
    "print(f\"Validation: AUC={auc:.4f}  F1@best={f1:.4f}  thr={thr:.2f}\")\n",
    "\n",
    "# Saveing bundle + meta (model + scaler bundled for reuse)\n",
    "import json as _json\n",
    "bundle_path = DATA_DIR / \"best_sgd_stream.joblib\"\n",
    "joblib.dump({\"scaler\": scaler, \"model\": clf}, bundle_path)\n",
    "(DATA_DIR / \"best_name.txt\").write_text(\"sgd_stream\")\n",
    "(DATA_DIR / \"best_meta.json\").write_text(_json.dumps({\"name\": \"sgd_stream\", \"thr\": float(thr)}, indent=2))\n",
    "print(\"Saved:\", bundle_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ae08b-ae78-4c64-bcd6-2710a058e337",
   "metadata": {},
   "source": [
    "### Generating submissions (raw sigmoid + Platt calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6159ccd-3607-49f6-ac23-8de2fb7d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: D:\\Adelaide\\2ndTrimester2 2025\\Using Machine Learning Tools_4536_COMP_SCI_X_0001\\A3\\submissions\\submission_expit.csv\n",
      "Wrote: D:\\Adelaide\\2ndTrimester2 2025\\Using Machine Learning Tools_4536_COMP_SCI_X_0001\\A3\\submissions\\submission_platt.csv\n",
      "RAW   probs -> min: 0.0 max: 1.0 mean: 0.2611111104488373 std: 0.43924033641815186\n",
      "PLATT probs -> min: 0.013428269701594017 max: 0.9990216546697579 mean: 0.39289330784319737 std: 0.30235123121346336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  probability\n",
       "0  15          0.0\n",
       "1  16          0.0\n",
       "2  17          0.0\n",
       "3  18          0.0\n",
       "4  19          0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0.203387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.466815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.059594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.234636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0.191696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  probability\n",
       "0  15     0.203387\n",
       "1  16     0.466815\n",
       "2  17     0.059594\n",
       "3  18     0.234636\n",
       "4  19     0.191696"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raw sigmoid + Platt-calibrated using validation\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Loading bundle\n",
    "bundle = joblib.load(DATA_DIR / \"best_sgd_stream.joblib\")\n",
    "scaler = bundle[\"scaler\"]; clf = bundle[\"model\"]\n",
    "\n",
    "# Building validation for Platt calibration\n",
    "VAL_PATH = resolve_jsonl(DATA_DIR / \"validation.jsonl\")\n",
    "vitems   = read_jsonl(VAL_PATH)\n",
    "y_val    = np.array([it[\"label\"] for it in vitems], dtype=int)\n",
    "X_val    = np.vstack([pool_stats_flex(it[\"features\"]) for it in vitems]).astype(np.float32)\n",
    "X_val    = scaler.transform(X_val).astype(np.float32, copy=False)\n",
    "val_scores = clf.decision_function(X_val).reshape(-1, 1)\n",
    "\n",
    "# Fitting calibrator\n",
    "calib = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
    "calib.fit(val_scores, y_val)\n",
    "\n",
    "# Streaming test once, compute raw scores\n",
    "TEST_PATH = resolve_jsonl(DATA_DIR / \"test_features.jsonl\")\n",
    "tids, scores, buf = [], [], []\n",
    "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "        obj = json.loads(line)\n",
    "        tids.append(obj[\"id\"])\n",
    "        buf.append(obj[\"features\"])\n",
    "        if len(buf) == BATCH_TEST:\n",
    "            Xb = np.vstack([pool_stats_flex(b) for b in buf]).astype(np.float32)\n",
    "            Xb = scaler.transform(Xb).astype(np.float32, copy=False)\n",
    "            scores.append(clf.decision_function(Xb))\n",
    "            buf = []\n",
    "    if buf:\n",
    "        Xb = np.vstack([pool_stats_flex(b) for b in buf]).astype(np.float32)\n",
    "        Xb = scaler.transform(Xb).astype(np.float32, copy=False)\n",
    "        scores.append(clf.decision_function(Xb))\n",
    "\n",
    "scores = np.concatenate(scores).reshape(-1, 1)\n",
    "\n",
    "# Two sets of probabilities\n",
    "proba_raw   = expit_stable(scores.ravel())         # raw sigmoid\n",
    "proba_platt = calib.predict_proba(scores)[:, 1]    # calibrated\n",
    "\n",
    "# Write both Kaggle files\n",
    "sub_raw   = pd.DataFrame({\"id\": tids, \"probability\": proba_raw})\n",
    "sub_platt = pd.DataFrame({\"id\": tids, \"probability\": proba_platt})\n",
    "\n",
    "out_raw   = SUBMIT_DIR / \"submission_expit.csv\"\n",
    "out_platt = SUBMIT_DIR / \"submission_platt.csv\"\n",
    "sub_raw.to_csv(out_raw, index=False)\n",
    "sub_platt.to_csv(out_platt, index=False)\n",
    "print(\"Wrote:\", out_raw)\n",
    "print(\"Wrote:\", out_platt)\n",
    "\n",
    "# Quick sanity-check\n",
    "print(\"RAW   probs ->\", \"min:\", float(proba_raw.min()),  \"max:\", float(proba_raw.max()),\n",
    "      \"mean:\", float(proba_raw.mean()),  \"std:\", float(proba_raw.std()))\n",
    "print(\"PLATT probs ->\", \"min:\", float(proba_platt.min()),\"max:\", float(proba_platt.max()),\n",
    "      \"mean:\", float(proba_platt.mean()),\"std:\", float(proba_platt.std()))\n",
    "\n",
    "display(sub_raw.head(), sub_platt.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e186921-c118-455c-9d89-5f7311fb1bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
